#疑问: 为何在训练模型时, 只需随机mask item, 测试时的效果不错? 不需要finetune步骤吗？
templates: []
mode: train
resume_training: false
pilot: false
pretrained_weights: null
num_users: null
num_items: null
num_ratings: null
num_days: null
local_data_folder: ./Data
dataset_code: beauty
min_rating: 0
min_uc: 5
min_sc: 5
split: leave_one_out
dataloader_code: bert_pair
dataloader_random_seed: 0.0
train_batch_size: 256
val_batch_size: 256
test_batch_size: 256
train_window: 100
dataloader_output_timestamp: false
dataloader_output_days: false
dataloader_output_user: false
train_negative_sampler_code: random
train_negative_sample_size: 0
train_negative_sampling_seed: 0
test_negative_sampler_code: popular
test_negative_sample_size: 100
test_negative_sampling_seed: 98765
trainer_code: bert
device: cuda
use_parallel: true
num_workers: 0
optimizer: Adam
lr: 0.001
weight_decay: 0.0 #ddd
momentum: null
decay_step: 25
gamma: 1.0
clip_grad_norm: 5.0
# num_epochs: -1
num_epochs: -1
log_period_as_iter: 12800
metric_ks:
- 1
- 5
- 10
- 20
- 50
best_metric: NDCG@10
saturation_wait_epochs: 20
# model_code: bert
model_code: bert_argu_input
pooling_type: mean-pooling
# pooling_type: cls_pooling
model_init_seed: 0
model_init_range: 0.02
# max_len: 200
max_len: 50
hidden_units: 64
# hidden_units: 128
num_blocks: 2
# num_heads: 2
num_heads: 4
dropout: 0.2
# mask_prob: 0.2
mask_prob: 0.6
output_info: false
residual_ln_type: post
headtype: dot
head_use_ln: true
time_unit_divide: null
freq: 10000
tisas_max_time_intervals: null
marank_max_len: null
marank_num_att_layers: null
marank_num_linear_layers: null
absolute_kernel_types: null
relative_kernel_types: null
experiment_root: experiments
experiment_group: test
# experiment_name: bert
experiment_name: bert_mean_pooling_beauty_shuffle_subseq_0_4_cl_weight_1_0 #输出结果的文件名; 上述三者构成输出结果的根目录; 例如/experiment_root/experiment_group/experiment_name
wandb_project_name: null
wandb_run_name: null
wandb_run_id: null
meta: training
constrastive_flag: true
constrastive_input_flag: true
constrastive_model_flag: false
# constrastive_weight: 0.1
constrastive_weight: 1.0
cutoff_rate: 0.1
cutoff_type: random
# argument_type: cutoff
# argument_type: cutoff_subseq
argument_type: shuffle_subseq
shuffle_subseq_rate: 0.4 #shuffle的子序列长度;
cutoff_subseq_rate: 0.6 #子序列长度比例;
dropout_rate: 0.2
finetune_flag: false
resume_training: false